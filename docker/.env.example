# T.H.U.V.U. Docker Environment Configuration
# ============================================
# Copy this file to .env and customize as needed

# ===========================================
# Database Configuration
# ===========================================
POSTGRES_PASSWORD=thuvu_secret

# ===========================================
# LLM Configuration
# ===========================================
# For local Ollama (default when using full profile):
LLM_HOST=http://ollama:11434
LLM_MODEL=qwen2.5-coder:14b

# For external APIs (e.g., DeepSeek, OpenAI-compatible):
# LLM_HOST=https://api.deepseek.com
# LLM_MODEL=deepseek-chat
# LLM_API_KEY=your-api-key-here

# For LM Studio on host machine:
# LLM_HOST=http://host.docker.internal:1234
# LLM_MODEL=your-model-name

# ===========================================
# Embedding Configuration
# ===========================================
# For local Ollama:
EMBEDDING_HOST=http://ollama:11434
EMBEDDING_MODEL=nomic-embed-text

# For external embedding service:
# EMBEDDING_HOST=https://api.openai.com
# EMBEDDING_MODEL=text-embedding-ada-002

# ===========================================
# GPU Configuration (for Ollama)
# ===========================================
# Uncomment to enable NVIDIA GPU support
# OLLAMA_GPU=nvidia

# ===========================================
# Web UI Configuration
# ===========================================
# Port for T.H.U.V.U. web interface
WEB_PORT=5000

# ===========================================
# Feature Toggles
# ===========================================
# Enable/disable RAG
RAG_ENABLED=true

# Enable/disable MCP code execution
MCP_ENABLED=true

# Auto-approve tool calls (set to false for more control)
AUTO_APPROVE_TOOLS=true
